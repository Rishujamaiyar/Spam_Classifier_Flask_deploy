{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam_classifier.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E1r4Gi8sS9n"
      },
      "source": [
        "### Importing required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r9_evQ1nWEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb89d0ab-a81a-461d-8b08-d1fb0408db79"
      },
      "source": [
        "#Importing Libraries\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cnKX1qeyAs0"
      },
      "source": [
        "### Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCEkqeS9obX2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "f390421f-f8e8-4bf0-9d91-a416354f8fb2"
      },
      "source": [
        "#Dataset Loading\n",
        "dataset = pd.read_csv('spam1.csv',names=['sentiment','statement'])\n",
        "dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>statement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>WINNER!! As a valued network customer you have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>spam</td>\n",
              "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>spam</td>\n",
              "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1313</th>\n",
              "      <td>ham</td>\n",
              "      <td>I'm working technical support  voice process.n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1314</th>\n",
              "      <td>ham</td>\n",
              "      <td>I might come to kerala for 2 days.so you can b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok. Not sure what time tho as not sure if can ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1316</th>\n",
              "      <td>ham</td>\n",
              "      <td>That's fine, I'll bitch at you about it later ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>ham</td>\n",
              "      <td>No my mum went 2 dentist.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1318 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentiment                                          statement\n",
              "0         spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "1         spam  FreeMsg Hey there darling it's been 3 week's n...\n",
              "2         spam  WINNER!! As a valued network customer you have...\n",
              "3         spam  Had your mobile 11 months or more? U R entitle...\n",
              "4         spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
              "...        ...                                                ...\n",
              "1313       ham  I'm working technical support  voice process.n...\n",
              "1314       ham  I might come to kerala for 2 days.so you can b...\n",
              "1315       ham  Ok. Not sure what time tho as not sure if can ...\n",
              "1316       ham  That's fine, I'll bitch at you about it later ...\n",
              "1317       ham                          No my mum went 2 dentist.\n",
              "\n",
              "[1318 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WAq2WdwvTe5"
      },
      "source": [
        "def cleanString(incomingString):\n",
        "    newstring = incomingString\n",
        "    newstring = newstring.replace(\"!\",\"\")\n",
        "    newstring = newstring.replace(\"@\",\"\")\n",
        "    newstring = newstring.replace(\"#\",\"\")\n",
        "    newstring = newstring.replace(\"$\",\"\")\n",
        "    newstring = newstring.replace(\"%\",\"\")\n",
        "    newstring = newstring.replace(\"^\",\"\")\n",
        "    newstring = newstring.replace(\"&\",\"and\")\n",
        "    newstring = newstring.replace(\"*\",\"\")\n",
        "    newstring = newstring.replace(\"(\",\"\")\n",
        "    newstring = newstring.replace(\")\",\"\")\n",
        "    newstring = newstring.replace(\"+\",\"\")\n",
        "    newstring = newstring.replace(\"=\",\"\")\n",
        "    newstring = newstring.replace(\"?\",\" \")\n",
        "    newstring = newstring.replace(\"\\'\",\"\")\n",
        "    newstring = newstring.replace(\"\\\"\",\"\")\n",
        "    newstring = newstring.replace(\"'\",\"\")\n",
        "    newstring = newstring.replace(\"'m\",\"am\")\n",
        "    newstring = newstring.replace(\"}\",\"\")\n",
        "    newstring = newstring.replace(\"[\",\"\")\n",
        "    newstring = newstring.replace(\"]\",\"\")\n",
        "    newstring = newstring.replace(\"<\",\"\")\n",
        "    newstring = newstring.replace(\">\",\"\")\n",
        "    newstring = newstring.replace(\"~\",\"\")\n",
        "    newstring = newstring.replace(\"`\",\"\")\n",
        "    newstring = newstring.replace(\":\",\"\")\n",
        "    newstring = newstring.replace(\";\",\"\")\n",
        "    newstring = newstring.replace(\"|\",\"\")\n",
        "    newstring = newstring.replace(\"\\\\\",\"\")\n",
        "    newstring = newstring.replace(\"/\",\"\") \n",
        "    newstring = newstring.replace(\"0\",\"\")\n",
        "    newstring = newstring.replace(\"1\",\"\")\n",
        "    newstring = newstring.replace(\"2\",\"\")\n",
        "    newstring = newstring.replace(\"3\",\"\")\n",
        "    newstring = newstring.replace(\"4\",\"\")\n",
        "    newstring = newstring.replace(\"5\",\"\")\n",
        "    newstring = newstring.replace(\"6\",\"\")\n",
        "    newstring = newstring.replace(\"7\",\"\")\n",
        "    newstring = newstring.replace(\"8\",\"\")\n",
        "    newstring = newstring.replace(\"9\",\"\")  \n",
        "    newstring = newstring.replace(\".\",\"\")\n",
        "    newstring = newstring.replace(\",\",\"\")\n",
        "    return newstring"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8OVy9A7wR2l"
      },
      "source": [
        "clean_text = []\n",
        "for col,row in dataset.iterrows():\n",
        "  text = cleanString(row[1])\n",
        "  text_tokens = word_tokenize(text)\n",
        "  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "  text = (\" \").join(tokens_without_sw)\n",
        "  clean_text.append(text)\n",
        "dataset['statement'] = clean_text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Nz4xmuwY3t"
      },
      "source": [
        "### Model training and Pickling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dVgreFk0Htj",
        "outputId": "e2eb0bea-089e-49f0-a684-ec4c64908dde"
      },
      "source": [
        "#Training the model and predicting\n",
        "#Creating Vocabulary\n",
        "id = dataset['statement']\n",
        "model = Tokenizer()\n",
        "model.fit_on_texts(list(id))\n",
        "\n",
        "X = dataset['statement']\n",
        "y = dataset['sentiment']\n",
        "\n",
        "\n",
        "X = model.texts_to_matrix(X, mode='count')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
        "gnb = GaussianNB()\n",
        "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "print((X_test.shape[0], (y_test == y_pred).sum()))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(264, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqKtbJTE65Fz"
      },
      "source": [
        "#Creating the classifier function for queries\n",
        "def classifier(text):\n",
        "  text_ = cleanString(text)\n",
        "  text_tokens = word_tokenize(text_)\n",
        "  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "  text = (\" \").join(tokens_without_sw)\n",
        "  vec = model.texts_to_matrix([text], mode='count')\n",
        "  return gnb.predict(vec)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhyJP9LN7lGN",
        "outputId": "d7d2c59c-8b00-432f-ac2a-83fddeb21dbb"
      },
      "source": [
        "print(classifier(\"Sorry, I'll call later\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ham']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvxNRf2quhbs",
        "outputId": "516d21b5-3395-4d05-c5d1-e85c9ba30a80"
      },
      "source": [
        "print(classifier(\"Congratulation! You won a Prize!\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['spam']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kSZg5bR6q1s"
      },
      "source": [
        "#Serialization the classifier using pickle\n",
        "pickle_out = open(\"classifier.pkl\",\"wb\")\n",
        "pickle.dump(classifier, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}